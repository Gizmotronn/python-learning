# Computers that can see

https://www.coursera.org/learn/uol-machine-learning-for-all/quiz/rweym/computers-that-see/attempt?redirectToCover=true

Humans (and a lot of other animals, especially mammals) are hardwired to be able to recognize each other. This is because we are social animals, and we gained our status as the dominant species on Earth because of our mass communication and cooperation skills. Because this was the only way for us to get to the top of the food chain (we have almost no true predator abilities at birth), we had to be able to recognize each other. This skill was hardwired into us. 



For example, if a tribe of humans were going on a mammal hunt, it is likely that there would have been some sort of plan put in place. Each member of the tribe would have to know their role and the roles of their fellow tribespeople. In contrast, while animals like lions definitely communicate in some capacity, their communication skills are much more limited and less necessary; a lion does not need to communicate with 40 other lions to be able to hunt and kill its prey. This is why, for humans at least, facial recognition and other social skills was so important.



For computers, this skill was not needed until very recently. Although the idea of the Turing Machine (pioneered by British scientist Alan Turing) had been created in the 1940's, most computers, for a long time, were given a single task to do. This may be adding up the cost of items in a basket, or scanning a barcode. Today, computers do a much wider range of tasks for us, and these computers are everywhere - laptops, smartphones, tablets and watches. Computers need to be able to communicate with each other (for example a computer "talking" via radio signals to the internet or a server), however this is very different to how humans communicate.



Two humans would communicate, not just by talking, but with a whole host of other actions as well. Both of them may raise their arms while they're talking to articulate what they are saying, and they're both checking in on the facial expressions of their partner. 



For 2 computers to talk to each other, a single transfer of data is all that is needed. 



For computers to learn to see in the same way as people, I believe that they will need to be able to analyze the manirisms with which different people communicate, and connect the facial expressions and the like with the mood (which can be detected to some extent today with special helmets). We humans can do this automatically - if someone is speaking gruffly to us, or they're raising their eyebrows, we automatically connect the dots and we may change our approach or what we're saying. Computers need to know when we do this, and why we do this.